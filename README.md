# Technical Assessment for Breakawai
### (Breakawai) Data Scientist Trainee - Technical Assessment
âš  Made with Python 3.10 and SQLite3 âš 


## Set Objectives
- Create an ETL script to prepare a dataset for analysis:
  - Join the 3 Procure-to-Pay CSV files into one dataset, ensuring the validity and quality of the data; (âœ…) \<2025-01-08\>
  - Add 2 new attributes to the dataset based on the Start Timestamp; (âœ…) \<2025-01-08\>
  - Add an extra attribute of the World Continent based on the country given. (âœ…) \<2025-01-09\>
  - Add a new dataset attribute to cluster the activities in phases, merging the dataset with the file "Phases_of_activities.csv" (âœ…) \<2025-01-09\>

- Create a Machine Learning script for adding value based on the existing transformed dataset:
  - Create an extra column in the dataset that clusters the data based multiple "unseen" patterns; (âœ…) \<2025-01-13\>
  - Explain the process and the decisions behind the machine learning techniques used to create this new column; (âœ…) \<2025-01-14\>
  - Present some visualization of the graphical clusters; (âœ…) \<2025-01-13\>

- Deadline \<2025-01-15\>

## Proposed Tasks
- Problem understanding:
  - Prepare Git (âœ…) \<2025-01-06\>
  - Define ETL tasks (âœ…) \<2025-01-08\>
  - Understand Data (âœ…) \<2025-01-07\>
  - Prepare ETL file hierarchy (âœ…) \<2025-01-06\>
  - Define ML tasks (âœ…) \<2025-01-10\>


- ETL pipeline:
  - Define an executor of commands in SQL (âœ…) \<2025-01-06\>
  - Define a Logger for the ETL process (âœ…) \<2025-01-06\>
  - Define a Config File Parser (âœ…) \<2025-01-07\>
  - Read CSVs (âœ…) \<2025-01-07\>
  
  - Add Atributes Quarter, Week, Continent (âœ…) \<2025-01-08\>
  - Validate Attributes, NAs, Duplicates, etc. (âœ…) \<2025-01-08\>
  
  - Clear CSV data (âœ…) \<2025-01-09\>
  - Define SQL Tables (âœ…) \<2025-01-09\>
  - Write SQL data (âœ…) \<2025-01-09\>
  - Merge Atributes and Cluster Phases (âœ…) \<2025-01-09\>


- ML Engineering:
  - Preprocess Data Types (âœ…) \<2025-01-10\>
  - Analyze Correlations (âœ…) \<2025-01-10\>
  - Reduce dimentionality and plot (âœ…) \<2025-01-10\>
  - Apply KMeans (âŒ) \<2025-01-10\>
  - Apply PCA (âœ…) \<2025-01-13\>
  - Apply KMeans (âœ…) \<2025-01-13\>
  - Analize Cluster (âœ…) \<2025-01-14\>
  - Explain Methods and Clusters (âœ…) \<2025-01-14\>

- Output:
  - Package Content (âœ…) \<2025-01-15\>

## Code Summary
- [Python](/02_Python) â‡’ Python Code folder with the ETL and ML files:

- [Aux Modules](/02_Python/auxiliary_modules) â‡’ Python Code with auxiliary scripts and functions

- [SQL](/03_SQL) â‡’ SQL queries and databases folder:

- [Data](/01_CSV) â‡’ Data folder with the required CSVs for execution and the output data;

- [Config](config.cfg) â‡’ Config file for global variables and adjustments to the script;

- [LOGS](/04_LOGS) â‡’ Logs produced by the script during execution;

- [LOGS](/05_IMAGES) â‡’ Output images produced by the script for cluster visualization;

- [Requirements](requirements.txt) â‡’ File with python packages requirements on the running system - "pip install -r requirements.txt" [Stack Overflow](https://stackoverflow.com/questions/7225900/how-can-i-install-packages-using-pip-according-to-the-requirements-txt-file-from);


## Results

- Main Output files will be on the CSV folder, folowing the name set at the config file +QN for quarter N;

- The scripts will also a DB file at the SQL folder;

- Images are resulting from the ML script, as well as an output csv with the Primary components variance;


## Legend
|xxxx-xx-xx| â†¦ To be addressed at

\<xxxx-xx-xx\> â†¦ Ended at

(ğŸš©) â†¦ TODO

(âœ…) â†¦ Done

(âŒ) â†¦ To be Reviewed




## Additional Resources

[Alpha3 to Country](https://www.kaggle.com/datasets/wbdill/country-codes-iso-3166) CSV by [Bdill](https://www.kaggle.com/wbdill) on Kaggle

[Country to Continent](https://www.kaggle.com/datasets/hserdaraltan/countries-by-continent) CSV by [Serdar Altan](https://www.kaggle.com/hserdaraltan) on kaggle
